{"name":"Developing a machine learning algorithm to predict activity quality","tagline":"","body":"\r\n# Step 1\r\n\r\nFirst I load the training and testing data\r\n```{r}\r\nsetwd(\"U:\\\\machine_learning\")\r\ntraining<-read.csv(\"pml-training.csv\")\r\ntesting<-read.csv(\"pml-testing.csv\")\r\nlibrary(caret)\r\n```\r\n# Step 2\r\n\r\nI select only the accellerometer, gyrometer and magnetometer readings as well as the ‘classe’ variable for my training dataset. I do the same for the testing data\r\n\r\n```{r}\r\ntraining1<-with(training,data.frame(accel_belt_x,accel_belt_y,\r\naccel_belt_z,gyros_belt_x,gyros_belt_y,gyros_belt_z,magnet_belt_x,\r\nmagnet_belt_y,magnet_belt_z,accel_arm_x,accel_arm_y,accel_arm_z,\r\ngyros_arm_x,gyros_arm_y,gyros_arm_z,magnet_arm_x,magnet_arm_y,\r\nmagnet_arm_z,accel_dumbbell_x,accel_dumbbell_y,accel_dumbbell_z,\r\ngyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,magnet_dumbbell_x,\r\nmagnet_dumbbell_y,magnet_dumbbell_z,accel_forearm_x,accel_forearm_y,\r\naccel_forearm_z,gyros_forearm_x,gyros_forearm_y,gyros_forearm_z,\r\nmagnet_forearm_x,magnet_forearm_y,magnet_forearm_z,classe))\r\n```\r\n# Step 3\r\n\r\nFor cross-validation I use bootstrap aggregating which results to bagging classification trees with 25 bootstrap replications.\r\n```{r} \r\nset.seed(12345)\r\n modFit<-train(training1$classe~.,method=\"treebag\",data=training1,\r\ntrControl=trainControl(method=\"cv\"))\r\n```\r\n# Step 4\r\n\r\nI predict future values for my training set and assess the model’s accuracy from the confusion matrix. It appears from the confusion matrix that the error rate is very small (close to zero)\r\n```{r}\r\npred1<-predict(modFit,training1)\r\nconfusionMatrix(training1$classe,pred1)\r\n```\r\n```{r}\r\n Confusion Matrix and Statistics\r\n \r\n           Reference\r\nPrediction    A    B    C    D    E\r\n          A 5580    0    0    0    0\r\n          B    0 3796    1    0    0\r\n          C    0    0 3422    0    0\r\n          D    0    0    3 3213    0\r\n          E    0    0    0    0 3607\r\n\r\nOverall Statistics\r\n                                     \r\n                Accuracy : 1         \r\n                 95% CI : (0.999, 1)\r\n     No Information Rate : 0.284     \r\n     P-Value [Acc > NIR] : <2e-16    \r\n                                     \r\n                   Kappa : 1         \r\n  Mcnemar's Test P-Value : NA        \r\n \r\n Statistics by Class:\r\n \r\n                      Class: A Class: B Class: C Class: D Class: E\r\n Sensitivity             1.000    1.000    0.999    1.000    1.000\r\n Specificity             1.000    1.000    1.000    1.000    1.000\r\n Pos Pred Value          1.000    1.000    1.000    0.999    1.000\r\n Neg Pred Value          1.000    1.000    1.000    1.000    1.000\r\n Prevalence              0.284    0.193    0.175    0.164    0.184\r\n Detection Rate          0.284    0.193    0.174    0.164    0.184\r\n Detection Prevalence    0.284    0.194    0.174    0.164    0.184\r\n Balanced Accuracy       1.000    1.000    0.999    1.000    1.000\r\n\r\n```\r\n\r\n# Step 5\r\n\r\nI predict future values for the testing set. I would expect the error on the testing set to be bigger than that of the training set.\r\n\r\n```{r}\r\npred2<-predict(modFit,testing1)\r\npred2\r\n```\r\n```{r}\r\n  [1] B A B A A E D B A A B C B A E E A B B B\r\n Levels: A B C D E\r\n```\r\n# Conclusion\r\nThe reasons I chose bagging for my prediction model are a) it was the only easy and straightforward cross-validation method I could use (I tried to use K-fold but I couldn’t figure out how) and b) I tried different models(a simple tree, a linear discriminant and a naive Bayes model)but they were not as accurate as the bagging ones. Moreover, ‘bagging’ results to similar bias and reduced variance which makes me feel more confident about my choise.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}