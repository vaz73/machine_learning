{"name":"Machine learning","tagline":"","body":"---\r\ntitle: \"Developing a machine learning algorithm to predict activity quality\"\r\nauthor: \"Vasilis Nikolaou\"\r\ndate: \"Wednesday, August 20, 2014\"\r\noutput: html_document\r\n---\r\n\r\n# Step 1\r\n\r\nFirst I load the training and testing data\r\n\r\n```{r, echo=TRUE}\r\nsetwd(\"U:\\\\machine_learning\")\r\ntraining<-read.csv(\"pml-training.csv\")\r\ntesting<-read.csv(\"pml-testing.csv\")\r\nlibrary(caret)\r\n```\r\n\r\n# Step 2\r\n\r\nI select only the accellerometer, gyrometer and magnetometer readings as well as the 'classe' variable for my training dataset. I do the same for the testing data\r\n```{r, echo=TRUE}\r\ntraining1<-with(training,data.frame(accel_belt_x,accel_belt_y,accel_belt_z,gyros_belt_x,gyros_belt_y,gyros_belt_z,magnet_belt_x,magnet_belt_y,magnet_belt_z,accel_arm_x,accel_arm_y,accel_arm_z,gyros_arm_x,gyros_arm_y,gyros_arm_z,magnet_arm_x,magnet_arm_y,magnet_arm_z,accel_dumbbell_x,accel_dumbbell_y,accel_dumbbell_z,gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,magnet_dumbbell_x,magnet_dumbbell_y,magnet_dumbbell_z,accel_forearm_x,accel_forearm_y,accel_forearm_z,gyros_forearm_x,gyros_forearm_y,gyros_forearm_z,magnet_forearm_x,magnet_forearm_y,magnet_forearm_z,classe))\r\n```\r\n \r\n```{r,echo=FALSE}\r\ntesting1<-with(testing,data.frame(accel_belt_x,accel_belt_y,accel_belt_z,gyros_belt_x,gyros_belt_y,gyros_belt_z,magnet_belt_x,magnet_belt_y,magnet_belt_z,accel_arm_x,accel_arm_y,accel_arm_z,gyros_arm_x,gyros_arm_y,gyros_arm_z,magnet_arm_x,magnet_arm_y,magnet_arm_z,accel_dumbbell_x,accel_dumbbell_y,accel_dumbbell_z,gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,magnet_dumbbell_x,magnet_dumbbell_y,magnet_dumbbell_z,accel_forearm_x,accel_forearm_y,accel_forearm_z,gyros_forearm_x,gyros_forearm_y,gyros_forearm_z,magnet_forearm_x,magnet_forearm_y,magnet_forearm_z))\r\n```\r\n\r\n# Step 3\r\n\r\nFor cross-validation I use bootstrap aggregating   which results to bagging classification trees with 25 bootstrap replications. \r\n\r\n```{r}\r\nset.seed(12345)\r\nmodFit<-train(training1$classe~.,method=\"treebag\",data=training1,trControl=trainControl(method=\"cv\"))\r\nprint(modFit$finalModel)\r\n```\r\n\r\n# Step 4\r\n\r\nI predict future values for my training set and assess the model's accuracy from the confusion matrix. It appears from the confusion matrix that the error rate is very small (close to zero)\r\n\r\n```{r}\r\npred1<-predict(modFit,training1)\r\nconfusionMatrix(training1$classe,pred1)\r\n```\r\n\r\n\r\n# Step 5\r\n\r\nI predict future values for the testing set. I would expect the error on the testing set to be bigger than that of the training set. \r\n\r\n```{r}\r\npred2<-predict(modFit,testing1)\r\npred2\r\n```\r\n\r\n# Conclusion\r\n\r\nThe reasons I chose bagging for my prediction model are a) it was the only easy and straightforward cross-validation method I could use (I tried to use K-fold but I couldn't figure out how) and b) I tried different models(a simple tree, a linear discriminant and a naive Bayes model)but they were not as accurate as the bagging ones. Moreover, 'bagging' results to similar bias and reduced variance which makes me feel more confident about my choise.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}